{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "from termcolor import colored\n",
    "from FeatureExtractor import get_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    print(colored(msg, 'blue'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_learning_rate(optimizer: torch.optim, epoch: int, total_epochs: int):\n",
    "    if epoch in [total_epochs * 0.6, total_epochs * 0.8]:\n",
    "        optimizer.param_groups[0]['lr'] *= 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_MoCo_feature_extractor(\n",
    "        temperature: float,\n",
    "        loader: torch.utils.data.DataLoader,\n",
    "        get_aug: Callable[[], Callable[[torch.Tensor], torch.Tensor]],\n",
    "        momentum: float,\n",
    "        key_dictionary_size: int,\n",
    "        num_epochs: int):\n",
    "    \"\"\"\n",
    "    Generates a feature extraction network as described by MoCo v2 paper based on the ResNet50 feature extractor backbone\n",
    "    :param temperature: hyperparameter defining the density of the contrastive loss function\n",
    "    :param loader: unlabeled training data loader\n",
    "    :param get_aug: augmentation function generator\n",
    "    :param momentum: hyperparameter defining the speed at which the key dictionary is updated\n",
    "    :param key_dictionary_size: hyperparameter defining the number of keys to maintain.  Should be a   product of the loader batch_size\n",
    "    :param num_epochs: number of epochs to train the MoCo feature extractor\n",
    "    :return: feature extraction network\n",
    "    \"\"\"\n",
    "\n",
    "    # f_q, f_k: encoder networks for query and key\n",
    "    # queue: dictionary as a queue of K keys (CxK)\n",
    "\n",
    "    # init\n",
    "    log(\"Initializing feature extractor training\")\n",
    "    f_q = get_encoder()\n",
    "    optimizer = torch.optim.SGD(f_q.parameters(), lr=0.03, weight_decay=1e-4, momentum=0.9)\n",
    "    f_k = copy.deepcopy(f_q)  # create independent copy of f_q that begins with the same parameters but updates more slowly\n",
    "\n",
    "    # Generate keys_queue\n",
    "    log(\"Generating initial keys queue\")\n",
    "    num_initial_key_batches = key_dictionary_size // loader.batch_size\n",
    "    keys_queue = torch.cat([\n",
    "        f_k(\n",
    "            get_aug()(next(loader))\n",
    "        )\n",
    "        for _ in tqdm(range(num_initial_key_batches))\n",
    "    ])\n",
    "\n",
    "    log(\"Beginning training loop\")\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        update_learning_rate(optimizer, epoch, num_epochs)\n",
    "        for x in loader: # load a minibatch x with N samples\n",
    "            x_q = get_aug()(x) # a randomly augmented version\n",
    "            x_k = get_aug()(x) # another randomly augmented version\n",
    "            q = f_q(x_q) # queries: NxC\n",
    "            k = f_k(x_k) # keys: NxC\n",
    "            k = k.detach() # no gradient to keys\n",
    "            minibatch_size, sample_size = k.shape\n",
    "            N, C, K = minibatch_size, sample_size, key_dictionary_size\n",
    "\n",
    "            # positive logits: Nx1\n",
    "            l_pos = torch.bmm(q.view(N,1,C), k.view(N,C,1))\n",
    "            # negative logits: NxK\n",
    "            l_neg = torch.mm(q.view(N,C), keys_queue.view(C,K))\n",
    "            # logits: Nx(1+K)\n",
    "            logits = torch.cat([l_pos, l_neg], dim=0)\n",
    "            print(f\"logits shape = {logits.shape}\")\n",
    "\n",
    "            # contrastive loss, Eqn.(1)\n",
    "            labels = torch.zeros_like(logits) # positives are the 0-th\n",
    "            loss = torch.nn.CrossEntropyLoss(logits/temperature, labels)\n",
    "\n",
    "            # SGD update: query network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # momentum update: key network\n",
    "            f_k.params = momentum*f_k.params + (1-momentum)*f_q.params  # FIXME: f_k.parameters()\n",
    "\n",
    "            # update dictionary\n",
    "            keys_queue = torch.cat((keys_queue, k)) # enqueue the current minibatch\n",
    "            keys_queue = keys_queue[k.shape[0]:] # dequeue the earliest minibatch\n",
    "\n",
    "    log(\"Completed training MoCo feature extractor!\")\n",
    "    return f_q"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}